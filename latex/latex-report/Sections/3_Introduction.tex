%================================================================
\section{Introduction}\label{sec:Introduction}
%================================================================

Differential equations have a wide range of applications in the natural sciences, and many methods have been developed for solving them. In numerical analysis, finite difference methods (FDMs) are widespread as they reduce the differential equation to a system of algebraic equations making the problem of finding the solution ideal to modern computers. The aim of this project is to design feed-forward neural network (FFNN) models suitable for solving ordinary, partial and coupled differential equations. 

The rationale for proposing an approach with FFNN models is justified by the universal approximation theorem, which states that feed-forward neural networks, when given appropriate parameters, can approximate continuous functions on compact subsets of $\R^n$. This includes the solution of many differential equations. Neural networks are uniquely suited in optimisation problems, and can be trained by various methods such as the many flavors of gradient descent algorithms. By reformulating the differential equation to an equation where minimization of some parameters must be done, a neural net might be able to solve the problem. To that end, a possible method is to formulate a trial solution involving the result from a neural network, and construct a cost function which contains information about derivatives of the network.

The first FFNN model is employed within the TensorFlow framework to learn the initial-boundary problem given by the heat equation, a partial differential equation (PDE) with both temporal and spatial dependence. The problem formulation is scaled to the standard unity interval (0,1) in one spatial dimension. An explicit numerical scheme using Forward Euler will be used to assess the efficiency and accuracy of the FFNN model, alongside the exact solution of the problem. 

The second FFNN model is employed, also within the TensorFlow framework, to learn the solution to the nonlinear, coupled ordinary differential equation (ODE) presented by Yi et. al in the article from \href{https://www.sciencedirect.com/science/article/pii/S0898122104901101}{Computers and Mathematics with Applications 47, 1155 (2004)}, which describes the state of a continuous-time recurrent neural network (CTRNN) model. Given a real symmetric matrix $A$ in the source term, the temporal dynamic described by this ODE has convergence properties to an eigenvector corresponding to the largest eigenvalue $\lambda$, given that the initial vector $\mathbf{x}_0$ is not orthogonal to the eigenspace of $\lambda$. If $\mathbf{x}_0$ is not orthogonal to the eigenspace of the smallest eigenvalue $\sigma$, replacing $A$ with $-A$ yields an eigenvector corresponding to $\sigma$. The eigenvalue itself is calculated with the Rayleigh quotient. The aim is to check if the constructed FFNN succeeds in computing both the largest and smallest eigenvalue for some benchmark $3\times 3$ and $6\times 6$ real symmetric matrices. We will also try choosing $\mathbf{x}_0$ orthogonal to the eigenspace corresponding to the largest eigenvalue, to make the network converge to an eigenvalue different from the largest. This starting point will itself be an eigenvector. In order to assess the FFNN model, we will compare the result with those from Euler's method for solving the same ODE and Numpy's linalg.eig which directly computes the eigenvalues of the matrix $A$. 

This project is structured by first presenting a theoretical overview of the heat equation and the eigenvalue problem, as well as the aforementioned statistical learning methods in \autoref{sec:Theory}. This is followed by a presentation on the approach to study the various computations of interest in \autoref{sec:Method}. Next, the results of the implementation are presented and discussed in \autoref{sec:Results}, before subsequently they are concluded upon in \autoref{sec:Conclusion}. Lastly, an outline of possible continuations of the models, with respect to the implementation, are presented in \autoref{sec:Future}.