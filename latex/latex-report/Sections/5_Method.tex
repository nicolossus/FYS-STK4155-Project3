%================================================================
\section{Method}\label{sec:Method}
%================================================================

%----------------------------------------------------------------
\subsection{The Heat Equation Problem}\label{sec:heat method}
%----------------------------------------------------------------

Following the discussion in \autoref{sec:Heat numerical Theory}, the computational algorithm \cite{hpl} for the Forward Euler explicit scheme becomes:
\begin{enumerate}
    \item Compute $u_i^0 = I(x)$ for $i=0, ..., N_x$
    \item for $n=0,1, ..., N_t$:
        \begin{itemize}
            \item[(a)] apply \autoref{eq:FE scheme} for all the internal spatial points $i=1, ..., N_x - 1$
            \item[(b)] set the boundary values $u_i^{n+1}=0$ for $i=0$ and $i=N_x$.
        \end{itemize}
\end{enumerate}

FFNN
Train on spatial and temporal points as dictated by FE stability criterion, Equal footing

\begin{figure}[H]
\begin{center}
\def\layersep{3cm}
\def\nodeinlayersep{1.5cm}
\begin{tikzpicture}[
   shorten >=1pt,->,
   draw=black!50,
    node distance=\layersep,
    every pin edge/.style={<-,shorten <=1pt},
    neuron/.style={circle,fill=black!25,minimum size=17pt,inner sep=0pt},
    input neuron/.style={neuron, fill=green!50},
    output neuron/.style={neuron, fill=red!50},
    hidden neuron/.style={neuron, fill=blue!50},
    annot/.style={text width=4em, text centered}
]
    % Draw the input layer nodes
    \foreach \name / \y in {1/3}
    % This is the same as writing \foreach \name / \y in {1/1,2/2,3/3,4/4}
        \node[input neuron, pin=left:Input $t$] (I-\name) at (0,-\y) {};
    % set number of hidden layers
    \newcommand\Nhidden{3}
    % Draw the hidden layer nodes
    \foreach \N in {1,...,\Nhidden} {
       \foreach \y in {1,...,12} {
          \path[yshift=6cm]
          node[hidden neuron] (H\N-\y) at (\N*\layersep,-\y*\nodeinlayersep ) {$\frac{1}{1+e^{-y}}$};
           }
    \node[annot,above of=H\N-1, node distance=1cm] (hl\N) {Hidden layer \N};
    }
    % Draw the output layer node
    \foreach \name / \y in {1,2,3,4,5,6}
        \node[output neuron,pin={[pin edge={->}]right:$x^{(\name)}$}, right of=H\Nhidden-6] (O-\name) at (9,-\y) {};
%
    % Connect every node in the input layer with every node in the
    % hidden layer.
    \foreach \source in {1}
        \foreach \dest in {1,...,12}
            \path (I-\source) edge (H1-\dest);
    % connect all hidden stuff
    \foreach [remember=\N as \lastN (initially 1)] \N in {2,...,\Nhidden}
       \foreach \source in {1,...,12}
           \foreach \dest in {1,...,12}
               \path (H\lastN-\source) edge (H\N-\dest);
    % Connect every node in the hidden layer with the output layer
    \foreach \source in {1,...,12}
        \foreach \name in {1,...,6}
        \path (H\Nhidden-\source) edge (O-\name);
    % Annotate the layers
    \node[annot,left of=hl1] {Input layer};
    \node[annot,right of=hl\Nhidden] {Output layer};
\end{tikzpicture}
\end{center}
\caption{The feed-forward neural network of choice in the case of $n=6$. Hidden layer 1 consists of $100$ nodes, hidden layer 2 consists of $50$ nodes, and hidden layer 3 consists of $25$ nodes. Sigmoid is the activation function for all hidden layers.}
\label{fig:ffnn_heat}
\end{figure}




%%%%%%%%


\begin{tikzpicture}[x=1.5cm, y=1.5cm, >=stealth]

\foreach \m/\l [count=\y] in {1,2,3,missing,4}
  \node [every neuron/.try, neuron \m/.try] (input-\m) at (0,2.5-\y) {};

\foreach \m [count=\y] in {1,missing,2}
  \node [every neuron/.try, neuron \m/.try ] (hidden-\m) at (2,2-\y*1.25) {};

\foreach \m [count=\y] in {1,missing,2}
  \node [every neuron/.try, neuron \m/.try ] (output-\m) at (4,1.5-\y) {};

\foreach \l [count=\i] in {1,2,3,n}
  \draw [<-] (input-\i) -- ++(-1,0)
    node [above, midway] {$I_\l$};

\foreach \l [count=\i] in {1,n}
  \node [above] at (hidden-\i.north) {$H_\l$};

\foreach \l [count=\i] in {1,n}
  \draw [->] (output-\i) -- ++(1,0)
    node [above, midway] {$O_\l$};

\foreach \i in {1,...,4}
  \foreach \j in {1,...,2}
    \draw [->] (input-\i) -- (hidden-\j);

\foreach \i in {1,...,2}
  \foreach \j in {1,...,2}
    \draw [->] (hidden-\i) -- (output-\j);

\foreach \l [count=\x from 0] in {Input, Hidden, Ouput}
  \node [align=center, above] at (\x*2,2) {\l \\ layer};

\end{tikzpicture}

%====================
%====================

\def\layersep{3cm}
\def\nodeinlayersep{1.5cm}
\begin{tikzpicture}[
   shorten >=1pt,->,
   draw=black!50,
    node distance=\layersep,
    every pin edge/.style={<-,shorten <=1pt},
    neuron/.style={circle,fill=black!25,minimum size=17pt,inner sep=0pt},
    input neuron/.style={neuron, fill=green!50},
    output neuron/.style={neuron, fill=red!50},
    hidden neuron/.style={neuron, fill=blue!50},
    annot/.style={text width=4em, text centered}
]
    % Draw the input layer nodes
    \foreach \name / \y in {1,...,3}
    % This is the same as writing \foreach \name / \y in {1/1,2/2,3/3,4/4}
        \node[input neuron, pin=left:Input \#\y] (I-\name) at (0,-\y) {};
    % set number of hidden layers
    \newcommand\Nhidden{2}
    % Draw the hidden layer nodes
    \foreach \N in {1,...,\Nhidden} {
       \foreach \y in {1,...,12} {
          \path[yshift=6cm]
          node[hidden neuron] (H\N-\y) at (\N*\layersep,-\y*\nodeinlayersep ) {$\frac{1}{1+e^{-x}}$};
           }
    \node[annot,above of=H\N-1, node distance=1cm] (hl\N) {Hidden layer \N};
    }
    % Draw the output layer node
    \node[output neuron,pin={[pin edge={->}]right:Output}, right of=H\Nhidden-6] (O) {};
%
    % Connect every node in the input layer with every node in the
    % hidden layer.
    \foreach \source in {1,...,3}
        \foreach \dest in {1,...,12}
            \path (I-\source) edge (H1-\dest);
    % connect all hidden stuff
    \foreach [remember=\N as \lastN (initially 1)] \N in {2,...,\Nhidden}
       \foreach \source in {1,...,12}
           \foreach \dest in {1,...,12}
               \path (H\lastN-\source) edge (H\N-\dest);
    % Connect every node in the hidden layer with the output layer
    \foreach \source in {1,...,12}
        \path (H\Nhidden-\source) edge (O);
    % Annotate the layers
    \node[annot,left of=hl1] {Input layer};
    \node[annot,right of=hl\Nhidden] {Output layer};
\end{tikzpicture}

%=====================
%=====================

%----------------------------------------------------------------
\subsection{The Eigenvalue Problem}\label{sec:eigen method}
%----------------------------------------------------------------

\autoref{thrm:eigenvector_convergence} gives a connection between solving differential equations and finding the largest and smallest eigenvalues of real symmetric matrices. We will explore this approach for finding eigenvalues by using the Euler method and the method for solving differential equations with FFNNs as described in \autoref{sec:solve_diff_eq_with_nn}.

Let the real symmetric matrix $A\in\mathbb{R}^{n\times n}$ be given. Our FFNN is set up with one node in the input layer, three hidden layers, and $n$ nodes in the output layer. The hidden layer next to the input has 100 nodes, the one in the middle has 50 nodes, and the last one has 25 nodes.

We define the function $f$ in \autoref{eq:diff_eigen_f} with $A$ or with $A$ replaced by $-A$, depending on whether we want to find the largest or the smallest eigenvalue of $A$.

If $N(t,P)$ denotes the output of the network at input $t$ with weights and biases contained in $P$, then our chosen trial function $g$ is given by
\begin{equation*}
    g(t) = \mathrm{e}^{-t}x_0 + (1-\mathrm{e}^{-t})N(t,P),
\end{equation*}
where $x_0\in\mathbb{R}^n$ is a chosen starting point of $g$. In the case where we want to find the largest eigenvalue $\lambda$ of $A$, $x_0$ has to be not orthogonal to the eigenspace corresponding to $\lambda$, and in the case where we want to find the smallest, say $\sigma$, the vector $x_0$ has to be not orthogonal to the eigenspace corresponding to $\sigma$.

We want to find the weights and biases $P$ such that $g$ is close to the solution of \autoref{eq:diff_eigen} at all points in an interval $[0,T]$, where $T$ is some fixed positive number. For this we choose points $0=t_0<t_1<\ldots<t_N=T$ and employ the Adam optimization algorithm to minimize the sum of squares
\begin{equation*}
    \sum_{i=0}^N(Dx(t_i)+x(t_i)-f(x(t_I)))^T(Dx(t_i)+x(t_i)-f(x(t_I))).
\end{equation*}

Finally, we use, by \autoref{thrm:eigenvector_convergence}, the vector $g(T)$ as an approximation to an eigenvector corresponding to the largest eigenvector $\lambda$. The value of $\lambda$ is then approximated by calculating the the Rayleigh quotient of $g(T)$, as given in \autoref{def:rayleigh_quotient}.

\begin{figure}[H]
\begin{center}
\def\layersep{3cm}
\def\nodeinlayersep{1.5cm}
\begin{tikzpicture}[
   shorten >=1pt,->,
   draw=black!50,
    node distance=\layersep,
    every pin edge/.style={<-,shorten <=1pt},
    neuron/.style={circle,fill=black!25,minimum size=17pt,inner sep=0pt},
    input neuron/.style={neuron, fill=green!50},
    output neuron/.style={neuron, fill=red!50},
    hidden neuron/.style={neuron, fill=blue!50},
    annot/.style={text width=4em, text centered}
]
    % Draw the input layer nodes
    \foreach \name / \y in {1/3}
    % This is the same as writing \foreach \name / \y in {1/1,2/2,3/3,4/4}
        \node[input neuron, pin=left:Input $t$] (I-\name) at (0,-\y) {};
    % set number of hidden layers
    \newcommand\Nhidden{3}
    % Draw the hidden layer nodes
    \foreach \N in {1,...,\Nhidden} {
       \foreach \y in {1,...,12} {
          \path[yshift=6cm]
          node[hidden neuron] (H\N-\y) at (\N*\layersep,-\y*\nodeinlayersep ) {$\frac{1}{1+e^{-y}}$};
           }
    \node[annot,above of=H\N-1, node distance=1cm] (hl\N) {Hidden layer \N};
    }
    % Draw the output layer node
    \foreach \name / \y in {1,2,3,4,5,6}
        \node[output neuron,pin={[pin edge={->}]right:$x^{(\name)}$}, right of=H\Nhidden-6] (O-\name) at (9,-\y) {};
%
    % Connect every node in the input layer with every node in the
    % hidden layer.
    \foreach \source in {1}
        \foreach \dest in {1,...,12}
            \path (I-\source) edge (H1-\dest);
    % connect all hidden stuff
    \foreach [remember=\N as \lastN (initially 1)] \N in {2,...,\Nhidden}
       \foreach \source in {1,...,12}
           \foreach \dest in {1,...,12}
               \path (H\lastN-\source) edge (H\N-\dest);
    % Connect every node in the hidden layer with the output layer
    \foreach \source in {1,...,12}
        \foreach \name in {1,...,6}
        \path (H\Nhidden-\source) edge (O-\name);
    % Annotate the layers
    \node[annot,left of=hl1] {Input layer};
    \node[annot,right of=hl\Nhidden] {Output layer};
\end{tikzpicture}
\end{center}
\caption{The feed-forward neural network of choice in the case of $n=6$. Hidden layer 1 consists of $100$ nodes, hidden layer 2 consists of $50$ nodes, and hidden layer 3 consists of $25$ nodes. Sigmoid is the activation function for all hidden layers.}
\label{fig:benchrun4}
\end{figure}

The FFNN model is employed within the \cw{TensorFlow} framework. Note that in the code the function call \cw{tf.GradientTape().gradient(y, x)} calculates the gradient of the sum of the components in \cw{y} with respect to the variables in \cw{x}.

We do a series of benchmark runs to check how well this neural network approach performs in regard to computing the largest and smallest eigenvalue for some $3\times 3$ and $6\times 6$ real symmetric matrices. We will also check the convergence of the network when $\mathbf{x}_0$ is chosen to be orthogonal to the eigenvector corresponding to the largest eigenvalue. In order to assess the performance, we will compare the result with those from Euler's method for solving the same ODE. We also compare the approximative eigenvalues and eigenvectors with those from \cw{Numpy's linalg.eig}.

For each benchmark problem, we decided on 2000 epoch for the Adam optimization algorithm. We always normalize the starting point $x_0$, as this made the convergence of the weights and biases of the network easier. Note that by the proof of \cite[Theorem 2]{yfh04}, the norm of $x(t),t\ge0$ is constant.

%==============================================================
\subsubsection{Benchmark Problem 1}\label{sec:benchmark problem 1}
%==============================================================
In this run we use the $3\times 3$ matrix
\begin{equation}\label{eq:33mat}
    A = \left(\begin{array}{ccc}
        3 & 2 & 4  \\
        2 & 0 & 2  \\
        4 & 2 & 3
    \end{array}\right),
\end{equation}
with eigenvalues $\lambda_1 = 8$ and $\lambda_2 = \lambda_3 = -1$. The initial point is $x_0=(1,0,0)$. We let $T=1$ and train the neural network on 5 evenly spaced timepoints in the interval $[0,T]$.

The Euler method is run with $10\,000$ time points, just to give a good approximation of the actual solution $x(t)$, for comparison with the neural network solution.

%==============================================================
\subsubsection{Benchmark Problem 2}\label{sec:benchmark problem 2}
%==============================================================
In this run we use the matrix
\begin{equation}\label{eq:33mat2}
    A = \left(\begin{array}{ccc}
        2 & 1 & 0 \\
        1 & 2 & 1 \\
        0 & 1 & 2
    \end{array}\right),
\end{equation}
with eigenvalues $\lambda_1 = 2+\sqrt{2}$, $\lambda_2 = 2$ and $\lambda_3 = 2-\sqrt{2}$.

We solve the differential equation for $-A$ to get an approximation of the smallest eigenvalue of $A$. We use $T=3.5$ as the end point of the domain of the solution.

%==============================================================
\subsubsection{Benchmark Problem 3}\label{sec:benchmark problem 3}
%==============================================================
This time we let
\begin{equation}\label{eq:33mat3}
    A = \left(\begin{array}{ccc}
        3 & 1 & -1 \\
        1 & 3 & -1 \\
        -1 & -1 & 5
    \end{array}\right),
\end{equation}
which has eigenvalues $\lambda_1 = 6$, $\lambda_2 = 2$ and $\lambda_3 = 3$.

Here we choose initial point $x_0=(-1, 1, 0)$, which is orthogonal to the eigenspace corresponding to the eigevalues $\lambda_1$ and $\lambda_3$ of $A$. We therefore expect $g(T)$ to approximate an eigenvector corresponding to the smallest eigenvalue. Note that since all the eigenvectors of $A$ are orthogonal, $x_0$ is itself an eigenvector, namely one corresponding to $\lambda_2$. We let $T=1$.

%==============================================================
\subsubsection{Benchmark Problem 4}\label{sec:benchmark problem 4}
%==============================================================
We now move on from $3\times 3$-matrices to a $6\times 6$-matrix. We use the randomly generated matrix $A$ which when rounded to four decimal places looks like
\begin{equation}\label{eq:66mat1}
    \left(\begin{array}{cccccc}
        0.4967 & 0.7205 & 0.4448 &  0.3075 & -0.3893 & -0.4179 \\
        0.4448 & -1.1914 & -1.7249 & 0.4517 & -1.0819 & 0.1503 \\
        0.3075 & -0.4349 & 0.4517 & -0.2258 &  0.2216 & -1.2412 \\
        -0.3893 & -0.1762 & -1.0819 & 0.2216 & -0.6006 & 0.2654 \\
        -0.4179 & 0.6933 & 0.1504 & -1.2412 & 0.2654 & -1.2208
    \end{array}\right),
\end{equation}
and has eigenvalues $\lambda_1 = -3.13078844$, $\lambda_2=1.82431291$, $\lambda_3 = 1.21134579$ $\lambda_4=-0.81694732$,  $\lambda_5=0.15215664$ and $\lambda_6=-1.74810717$. The initial point is randomly chosen as
\begin{equation*}
    x_0 = (0.20819531, 0.34735924, 0.3651539,  0.12346636, 0.6475964,  0.51771987).
\end{equation*}
We solve the differential equation with $A$ and approximate the largest eigenvalue. We do this for different number of time steps $N_t=5, 11,51,101$. The end point of the domain $[0,T]$ of the solution is $T=10$ for each run.

Here we also want to assess the performance of Euler, and hence we let the number of time steps we use for this method also vary. The number of time steps we consider are $N=51,101,501,1001$.
