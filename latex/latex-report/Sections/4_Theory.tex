%================================================================
\section{Theory}\label{sec:Theory}
%================================================================

%================================================================
\subsection{The Heat Equation}\label{sec:Heat eq Theory}
%===============================================================
In the natural sciences, problems with variables constrained by boundary conditions and initial values are often modelled by differential equations. The heat equation, which is a partial differential equation and a special case of the diffusion equation, describes the heat propagation in the evolution of time as it is transferred from regions of higher temperature to regions of lower temperature in a solid medium \cite{hpl}. The general heat equation reads
\begin{equation}\label{eq:heq general}
    \dot{u} = \alpha \nabla^2 u
\end{equation}
where $\alpha$ is the diffusion coefficient which is a measure of the rate of transfer of heat with dimension length$^2$/time.

The initial condition gives the temperature distribution
in the rod at t=0. Physically this means that we need to know the
temperature in the rod at a moment to be able to predict the future temperature distribution

Dirichlet conditions:
The temperatures at the endpoints of the rod,
T(0,t) and T(1,t), are prescribed at all time
Physically, this corresponds to a situation where we have a heat source which keep the temperature at given values at the endpoints


The complete initial-boundary value heat diffusion problem in one spatial dimension can then be specified as
\begin{alignat}{2}
    \pdv{u}{t} &= \alpha \pdv[2]{u}{x}, \quad &&x \in (0,L), t\in (0, T], \label{eq:heat eq 1D}\\
    u(x,0) &= I(x), \quad &&x\in [0,L], \\
    u(0,t) &= 0, \quad &&t \geq 0, \\
    u(L,t) &= 0, \quad &&t \geq 0
\end{alignat}

%================================================================
\subsection{Closed Form Solution of the Heat Equation}\label{sec:Heat analytic Theory}
%================================================================

Scale problem


\begin{equation}\label{eq:heat}
\frac{\partial^2 u(x,t)}{\partial x}= \frac{\partial u(x,t)}{\partial t},\;\;\;\;t>0,x\in[0,1],
\end{equation}
with initial conditions
\begin{equation*}
    u(x,0)=\sin(\pi x),\;\;\;\;0<x<1
\end{equation*}
and boundary conditions
\begin{equation*}
    u(0,t)=0,\;\;\;\;t\ge0
\end{equation*}
and
\begin{equation*}
    u(1,t)=0,\;\;\;\;t\ge0.
\end{equation*}

Assume that the solution has the form
\begin{equation*}
    u(x,t)=f(x)g(t).
\end{equation*}
By the initial conditions, we get that
\begin{equation*}
    f(x)=k^{-1}\sin(\pi x),
\end{equation*}
where $k=g(0)$. Then, from \cref{eq:heat},
\begin{equation*}
    f''(x)g(t) = f(x)g'(t)\;\;\;\Leftrightarrow\;\;\;\frac{f''(x)}{f(x)}=\frac{g'(t)}{g(t)}.
\end{equation*}
Plugging in for $f(x)$ and $f''(x)=-\pi^2a^{-1}\sin(\pi x)$, we find that
\begin{equation*}
    -\pi^2 = \frac{g'(t)}{g(t)}.
\end{equation*}
We integrate both sides with respect to $t$ to obtain
\begin{equation*}
    -\pi^2t+C = \int\frac{dg}{dt}\frac{1}{g}dt = \int\frac{1}{g}dg =\log |g|.
\end{equation*}
Hence
\begin{equation*}
    g(t)=D\mathrm{e}^{-\pi^2 t},
\end{equation*}
where $D$ is some constant. Since $g(0)=D$, we have $D=k$. We get that
\begin{equation*}
    u(x,t)=g(t)f(x)=\mathrm{e}^{-\pi^2 t}\sin(\pi x),
\end{equation*}
which can be checked to be a solution of \cref{eq:heat} satisfying the initial and boundary conditions. Next we show that this solution is unique.

Suppose that that $u_1$ and $u_2$ are solutions. Define $v(x,t)=u_1(x,t)-u_2(x,t)$ and let
\begin{equation*}
    w(t)=\frac{1}{2}\int_0^1v(x,t)^2\,\dd x,\;\;\;\; t\ge0.
\end{equation*}
Note that by \cref{eq:heat},
\begin{equation*}
    \frac{\partial^2 u(x,t)}{\partial x}= \frac{\partial u(x,t)}{\partial t},\;\;\;\;t>0,x\in[0,1],
\end{equation*}
by the initial condition $w(0)=0$, and by the boundary condition $v(0,t)=v(1,t)=0$ for $t>0$.

By Leibniz' rule \cite[8.11.2]{die69}, we can differentiate under the integral sign, so that
\begin{equation*}
    w'(t) = \int_0^1v(x,t)\pdv{v(x,t)}{t}\,\dd x = \int_0^1 v(x,t)\pdv[2]{v(x,t)}{x}\,\dd x.
\end{equation*}
Then integration by parts yields
\begin{equation*}
    w'(t) = \eval{v(x,t)\pdv{v(x,t)}{x}}_{x=0}^1 - \int_0^1\qty(\pdv{v(x,t)}{x})^2\,\dd x = -\int_0^1\qty(\pdv{v(x,t)}{x})^2\dd x,
\end{equation*}
implying that $w'(t)\le 0$ for all $t>0$. On the other hand, since $w(t)\ge0$ for all $t>0$ and $w(0)=0$, we must have $w'(t)\ge0$. It follows that $w'(t)=0, t>0$, and thus $w(t)=0$ for all $t\ge0$. We conclude that $u_1=u_2$.


%================================================================
\subsection{Explicit Numerical Scheme Using Forward Euler}\label{sec:Heat numerical Theory}
%================================================================
This section is based on the procedure described in \cite{hpl}. We have here opted for deriving a scheme for the non-scaled PDE to obtain more flexibility in the numerical model.

In order to derive the discrete set of equations to be solved, the domain $\qty[0,L]\times \qty[0,T]$ is discretized by replacing both the spatial and temporal domains by a set of uniform mesh points
\begin{align*}
    x_i &= i \Delta x, \quad i=0, ..., N_x
    \intertext{and}
    t_n &= n \Delta t, \quad n=0, ..., N_t
\end{align*}
The mesh function, denoted $u_i^n$, then approximates the exact solution $u\qty(x_i, t_n)$ for $i=0, ..., N_x$ and $n=0, ..., N_t$. Requiring the heat PDE in \autoref{eq:heat eq 1D} to be satisfied at each mesh point $\qty(x_i, t_n)$, $i=0, ..., N_x$ and $n=0, ..., N_t$, leads to the equation
\begin{equation*}
    \pdv{t} u\qty(x_i, t_n) = \alpha \pdv[2]{x} u \qty(x_i, t_n)
\end{equation*}
Replacing the first-order derivative with a forward difference approximation in time and the second-order derivative with a central difference approximation in space yields
\begin{equation*}
    \frac{u_i^{n+1}-u_i^n}{\Delta t} = \alpha \frac{u_{i+1}^n - 2u_i^n + u_{i-1}^n }{\Delta x^2},
\end{equation*}
which is the PDE turned into algebraic equations. We assume that the only unknown quantity is $u_i^{n+1}$. Solving with respect to this unknown gives

\begin{equation}\label{eq:FE scheme}
    u_i^{n+1} = u_i^n + F \qty(u_{i+1}^n - 2u_i^n + u_{i-1}^n),
\end{equation}
where $F=\alpha \Delta t/\Delta x^2$ is a dimensionless number. The stability criterion for the explicit scheme requires that $F \leq 1/2$. 

%================================================================
\subsection{Solving Differential Equations With Neural Networks}
%================================================================

\begin{definition}[Squashing function {\cite[Definition 2.3]{hsw89}}]
A \emph{squashing function} is a function $\psi\colon\mathbb{R}\to[0,1]$ that is monotonely increasing and with $\lim_{t\to\infty}\psi(t)=1$ and $\lim_{t\to -\infty}=0$.
\end{definition}

Denote by $\Sigma^n(\psi)$ the set of all output functions for single hidden layer feed-forward networks with $n$ real-valued inputs, having $\psi$ as squashing function at the hidden layer, and no squashing at the output. That is, $\Sigma^n(\psi)$ is the collection of functions $\phi\colon\mathbb{R}^n\to\mathbb{R}$ of the form
\begin{equation*}
    \phi(x)=\sum_{i=1}^m\beta_i\psi(w^T_ix + b_i),\;\;\;\;x,w_i\in\mathbb{R}^n,\beta_i,b_i\in\mathbb{R},m\in\mathbb{N}.
\end{equation*}

Given a subspace $K\subseteq\mathbb{R}^n$, let $C(K)$ be the set of all continuous functions from $K\subseteq\mathbb{R}^n$ into $\mathbb{R}$.

\begin{theorem}[Universal Approximation Theorem {\cite[Theorem 2.4]{hsw89}}]\label{thrm:universal_approximation}
For any squashing function $\psi$ and any $n\in\mathbb{N}$ and $K\subseteq\mathbb{R}^n$ compact, the set $\Sigma^n(\psi)$ is uniformly dense in $C(K)$.
\end{theorem}

From \autoref{thrm:universal_approximation} it follows that we can approximate the solutions of differential equations on compact subsets of $\mathbb{R}^n$ with neural networks.

In the following, suppose for simplicity that $f$ is a function from a compact interval $[t_0,t_1]$ of $\mathbb{R}$ into $\mathbb{R}$. A similar discussion applies to $f$ with range $\mathbb{R}^n$ and domain in $\mathbb{R}^n$ with any $n,m\in\mathbb{N}$. Let $f$ be given by a differential equation
\begin{equation*}
    G(t,f'(t),f''(t),\ldots,f^{(k)}(t)) = 0
\end{equation*}
for some $k\in\mathbb{N}$ and all $t$ in the interior of $K$. Suppose further that $f(t_0)=y_0$. To (approximately) solve this using a neural network $N$, the solution $g$ will be of the form
\begin{equation*}
    g(t) = h(t) + F(t)N(t,P),\;\;\;\;t\in K.
\end{equation*}
Here $N(x,P)$ denotes the output of the neural network at input $t$ with weights and biases contained in $P$. The functions $h$ and $F$ are chosen such that $h$ satisfies the initial condition (i.e. $h(t_0)=y_0$), and $F(t)$ is zero for the initial $t$ (i.e. $F(t_0)=0$). This ensures that the solution $g$ satisfies the initial condition.

To find the weights $P$, we choose some points $t_1,\ldots,t_N\in K$ and minimize the squared sum
\begin{equation*}
    \sum_{i=0}^N(G(t_i,g'(t_i),g''(t_i),\ldots g^{(k)}(t_i)))^2.
\end{equation*}


%----------------------------------------------------------------
\subsection{Project Theory 1}\label{sec:project theory}
%----------------------------------------------------------------


Let $A\in\mathbb{R}^{n\times n}$ be a real symmetric matrix. Define $f\colon\mathbb{R}^n\to\mathbb{R}^n$ by
\[
f(x)=[x^TxA+(1-x^TAx)I]x,\;\;\;\; x\in\mathbb{R}^n,
\]
where $I\in\mathbb{R}^{n\times n}$ is the identity matrix. Let $x\colon\mathbb{R}\to\mathbb{R}^n$ be a map that satisfies
\begin{equation}\label{eq:diff_eigen}
    Dx(t)=-x(t)+f(x(t)).
\end{equation}
We then have the following theorem \cite{yfh04}:
\begin{theorem}\label{thrm:eigenvector_convergence}
For each solution $x\colon\mathbb{R}\to\mathbb{R}^n$ of \autoref{eq:diff_eigen}, the limit $\lim_{t\to\infty}x(t)$ exists and converges to an eigenvector of $A$.

If $\lambda$ is the largest eigenvalue of $A$ and the starting point $x(0)$ is not orthogonal to the eigenspace of $\lambda$, then $\lim_{t\to\infty}x(t)$ is an eigenvector of $A$ with eigenvalue $\lambda$.

Replacing $A$ with $-A$, then if $x(0)$ is not orthogonal to the eigenspace of the smallest eigenvalue $\sigma$ of $A$, the limit $\lim_{t\to\infty}x(t)$ converges to an eigenvector corresponding to $\sigma$.
\end{theorem}

\begin{definition}[Rayleigh quotient {\cite[234]{hj13}}]\label{def:rayleigh_quotient}
Given a matrix $A\in\mathbb{R}^{n\times n}$ and a vector $x\in\mathbb{R}^n$, the \emph{Rayleigh quotient} is defined as
\begin{equation*}
    r(A,x) = \frac{x^TAx}{x^Tx}.
\end{equation*}
Note that if $x$ is an eigenvector of $A$ with eigenvalue $\lambda$, then
\begin{equation*}
    r(A,x)=\frac{x^TAx}{x^Tx}=\frac{x^T (\lambda x)}{x^T x}=\lambda.
\end{equation*}
\end{definition}