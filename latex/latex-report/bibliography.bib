% For style examples, see e.g. https://verbosus.com/bibtex-style-examples.html                    or                    https://www.economics.utoronto.ca/osborne/latex/BIBTEX.HTM

@book
{
    Sakurai,
    AUTHOR = {Sakurai, J. J. and Napolitano, J.},
     TITLE = {Modern Quantum Mechanics},
 PUBLISHER = {Cambridge University Press},
      YEAR = {2017},
      EDITION = {2},
      ISBN = {978-1-108-42241-3 Hardback},
      ADDRESS = {Cambridge, United Kingdom}
}


@online{PROJtwo,
  author = {Haug, N. and Netskar, T. K. and Wold, K.},
  title = {FYS-STK4155 Project 2 - Classifying Phases of the 2D Ising Model with Logistic Regression and Deep Neural Networks},
  %year = 1999,
  url = {https://github.com/nicolossus/FYS-STK4155-Project2/blob/master/report/FYS_STK4155_Project2.pdf},
  urldate = {2019-12-19}
}


@book
{
    hpl,
    AUTHOR = {Langtangen, H.P.},
     TITLE = {Finite difference methods for diffusion processes},
      YEAR = {2016},
      SERIES={Lecture notes for the course IN5270 at University of Oslo}
}


@book
{
  die69,
  title={Foundations of Modern Analysis},
  author={Dieudonn{\'e}, J.},
  number={v. 1;v. 10},
  lccn={73010084},
  series={Foundations of Modern Analysis},
  year={1969},
  publisher={Academic Press}
}

@article{yfh04,
title = "Neural networks based approach for computing eigenvectors and eigenvalues of symmetric matrix",
journal = "Computers \& Mathematics with Applications",
volume = "47",
number = "8",
pages = "1155 - 1164",
year = "2004",
issn = "0898-1221",
doi = "https://doi.org/10.1016/S0898-1221(04)90110-1",
url = "http://www.sciencedirect.com/science/article/pii/S0898122104901101",
author = "Zhang Yi and Yan Fu and Hua Jin Tang",
keywords = "Recurrent neural networks, Eigenvalues, Eigenvectors, Eigenspace, Symmetric matrix",
abstract = "Efficient computation of eigenvectors and eigenvalues of a matrix is an important problem in engineering, especially for computing eigenvectors corresponding to largest or smallest eigenvalues of a matrix. This paper proposes a neural network based approach to compute eigenvectors corresponding to the largest or smallest eigenvalues of any real symmetric matrix. The proposed network model is described by differential equations, which is a class of continuous time recurrent neural network model. It has parallel processing ability in an asynchronous manner and can achieve high computing performance. This paper provides a clear mathematical understanding of the network dynamic behaviors relating to the computation of eigenvectors and eigenvalues. Computer simulation results show the computational capability of the network model."
}

@book
{
  hj13,
  title={Matrix Analysis},
  author={Roger A. Horn and Charles R. Johnson},
  isbn={9780521839402 Hardback},
  year={2013},
  edition={2},
  publisher={Cambridge University Press}
}

@article{hsw89,
title = "Multilayer feedforward networks are universal approximators",
journal = "Neural Networks",
volume = "2",
number = "5",
pages = "359 - 366",
year = "1989",
issn = "0893-6080",
doi = "https://doi.org/10.1016/0893-6080(89)90020-8",
url = "http://www.sciencedirect.com/science/article/pii/0893608089900208",
author = "Kurt Hornik and Maxwell Stinchcombe and Halbert White",
keywords = "Feedforward networks, Universal approximation, Mapping networks, Network representation capability, Stone-Weierstrass Theorem, Squashing functions, Sigma-Pi networks, Back-propagation networks",
abstract = "This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators."
}